Continuation of transformers

One of the most challenging part of llm and it is gross. It is the heart of many problems in LLM.

How do we represent vocabulary as tokens?

Previous example in transformers/ tokenizes characters, which is very naive.

The paper language models are unsupervised multitask learners has an example of tokenization in section 2.2

Note: Use the same env as transformers.
